{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compelling Title\n",
    "_and witty tagline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Science 420 BB\n",
    "#### University of Washington Professional & Continuing Education\n",
    "#### Homework 5: Support Vector Machines\n",
    "#### Leo Salemann, 5/22/18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "**Explain the differences between traditional two-class Support Vector Machines (SMVs) versus two-class Locally-Deep Support Vector Machines (LD-SVMs). Provide examples of when you will prefer to use one over the other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Class SVMs in General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both types of Support Vector Machines (SVMs, LD-SVMs) are designed to solve binary classification problems by finding a hyperplane that separates the input data into two classes. If the data is two-dimensional (only two features per observation), the hyperplane becomes a line; for three dimensional data it beocmes a regular 2D plane, and for N-dimensions (data with N features per observation) it beocmes an N-1 dimensional hyperplane. The key point here is that SVMs are _linear_ classifiers - meaning that the line mentioned for the 2D case the hyperplane is an actual straight line not a curve; for the 3D case it's a flat 2D plane not a wavy surface; and for higher dimensions the hyperplane is still \"flat\" in whatever sense that may mean in N-1 space. In cases where the data doesn't lend it self to classificaiton by a \"flat\" separate (i.e. most of the time) a _Kernel Function_ is employed to project the data into a higher dimensional space in which a \"flat\" separator works. \n",
    "\n",
    "Consider a random collection og pixels sampled from an image of a fried egg. We have [x, y, color] and want to clasify yolk (round, in the center)  vs. egg white (backgroud, everythere else). No straight line will separate our yolk from white, but if we project our dataset into 3D space, the yolk becomes a \"mountain\" or \"hole\" and a flat plane will \"slice it\" from the egg white quite neatly.  \n",
    "\n",
    "SVM's offer a variety of kernel functions to (Polynomial, Gaussian, Sigmoid, RBD, ...) to acommplish feats like this in multi-dimensional space.  The tricky part is finding the right one, and putting in the time to test & tune it.  Got a big data set, hope you have patientce. These kernels can be slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What's so special about LD-SVMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Locally-Deep Support Vector Machines (LD-SVMs)](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-locally-deep-support-vector-machine) are essentially SVM's driven by [Local Deep Kernel Learning (LKDL)](http://manikvarma.org/code/LDKL/download.html), a kernel technique that addresses the projection needs descibed in the previous seciton, in a more efficent manner. LD-SVMs trade a bit of classification accuracy (usually less than a percentage point, occasionaly up to five) in favor of mulitple orders of magnitude peformance speedups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose, which one to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who worship speed (or shiny new things in general) may be tempted to go all-in with LD-SVMs right of the bat, but a few preliminary considerations can guide you to a wiser choice.\n",
    "\n",
    "**Do you have infrastructure constraints?** If you want to get started with readymade LD-SVMs, Azure ML Stodio is pretty much the only game in town (as of May 2018). You'll have a hard time finding R packages or Python libaries to help you, though you might be able to build a wrapper over [Varma et. all's LKDL Kerel implemented in C++](http://manikvarma.org/code/LDKL/LDKL.zip). Even if you have complete freedom with respect to choice of language, you may still have Cloud vs. On-Premesis constraints. Understand your client, data sets, and terms of data use before building pipelines you're not allowed to use. Keep up with cloud security data proteciton capabilities (multiple providers provide government-level proteciton), or look into [how to run the Azure stack On-Premises](https://azure.microsoft.com/en-us/resources/videos/microsoft-azure-stack-azure-services-on-premises/) \n",
    "\n",
    "**How big is your data?** LD-SVM sings when up against tens or even hundreds of thousands observaations with tens or hundreds of dimensions. \n",
    "\n",
    "**Can you tolerate a loss of accuracy?** LD-SVM will usually cost you some accuracy in favor of speed. [the characteristics of your data [Jain et al fig. 2](http://manikvarma.org/code/LDKL/download.html) as well as your tuning pareamters can have en effeect. If every once of accruacy counts, consider staying with conventional SVM.\n",
    "\n",
    "**Have you tried the easy way first?** Most clients appreciate partial progress now rather than a complete solution never. Unless your data is aboslutely massive, you can probably try a few standard kernel functions with conventional SVM, in the language and on the infrastructure of your choice, to see where you stand. Step up to LD-SVM if conventional SVM fails you.\n",
    "\n",
    "Finallly, [keep an eye on the forums](https://social.msdn.microsoft.com/Forums/azure/en-US/b809782e-2169-4aa0-b512-18e6542e6600/when-would-you-use-two-class-locally-deep-svm-vs-normal-svm?forum=MachineLearning) to see if the situation changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Class Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic idea is to separate a data set (a set of observations) into two classes (e.g. sick vs. healthy). \n",
    "\n",
    "If each observation has only two features (i.e. 2 dimensional) you can think of it as drawing a line through a scatterplot. If there's 3 features (i.e. dimensions0 the line becomes a plane.  Beyond that, it becomse an n-dimensional hyperplane separating data which rsides in n+1 dimensional space.\n",
    "\n",
    "Talk about linear classifiers ....\n",
    "\n",
    "Talk about large-margin decision boundaries ...\n",
    "\n",
    "Talk about kernel tricks and projections into higher dimensions (project the plane nto a cone)\n",
    "\n",
    "Talk about linear vs. non-linear ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Class Locally-Deep Support Vector Machines (MSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "google up some search terms, read it and write it\n",
    "\n",
    "Use it when linear classifyier doesn't perform well, or non-linear takes too long to train.\n",
    "\n",
    "Designed for faster training, non-linear classification tasks\n",
    "\n",
    "Tunable for single parameter or paramter range\n",
    "\n",
    "Can specify depth of tree ...\n",
    "\n",
    "Lambda W for L2 regularation to avoide overfitting when there's not enough features ...\n",
    "\n",
    "\n",
    "Lambda Theta for region space/thicness ...\n",
    "\n",
    "Sigmoid Sharpness\n",
    "\n",
    "Feature Normalizer\n",
    "\n",
    "\n",
    "More details from Manik Varma et. all's page on [LDKL: Local Deep Kernel Learning](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-locally-deep-support-vector-machine)\n",
    "\n",
    "\n",
    "\n",
    "Source: [Microsoft Azure article on Two-Class Locally Deep Support Vector Machines](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/two-class-locally-deep-support-vector-machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which SVM to use for What"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Classifying some cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autosdf = read.csv(\"veh-prime.csv\")\n",
    "head(autosdf, 3)\n",
    "dim(autosdf)\n",
    "summary(autosdf)\n",
    "str(autosdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace string-based target variable with a numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autosdf$TARGET <- ifelse(autosdf$CLASS =='car',1,-1)\n",
    "autosdf$CLASS <- NULL\n",
    "str(autosdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split into Test v. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Work in 10-fold Cross-Validation Somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Two-Class Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using package e1071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the package\n",
    "install.packages(\"e1071\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the library\n",
    "# Note that e1071 contains other classification and clustering methods and is a useful library to keep in your toolbox.\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm i sused to train a support vector machine\n",
    "# Various arguments are interesting - In this case you will see that we are using a linear kernel\n",
    "# You can also explore other kerney and observe how the decision boundary changes.\n",
    "# Various kernels are available - linear, polynomial, radial basis, and sigmoid.\n",
    "\n",
    "\n",
    "# You will also notice that we are using a cost = 10. \n",
    "# Cost refers to the C constant of the regularization term in a Lagrang formulae\n",
    "svmfit<-svm(TARGET~., data=autosdf, kernel=\"linear\", cost=10, scale=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "# plot(svmfit , autosdf)\n",
    "svmfit$index\n",
    "summary (svmfit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using a smaller cost value and see how it impacts the decision boundary\n",
    "svmfit<-svm(TARGET~., data=autosdf, kernel=\"linear\", cost=0.1, scale=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(svmfit , autosdf)\n",
    "svmfit$index\n",
    "summary (svmfit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Two-Class LocallyDeep Support Vector Machine (MSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
